---
title: "Carrefour Kenya"
author: "Christine Muthee"
date: "12/3/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Marketing Analysis.

## 1. Main objective: 
#### * Getting the most relevant marketting strategies that will result in the highest number of sales ( Total price tax inclusive)
### Specific Objectives
#### * Reduce highly dimensional datasets to lower dimensions for easier interpratability and analysis while assessing the attributes carrying core information in our dataset.
#### * Getting important attributes from the dataset.
#### * Getting associations within the transactions that will uncover important relationships for effective marketing.
#### * Checking for anomalies in the sales dataset for the sole purpose of fraud detection.

## 2. Data Inspection.
```{r}
library(data.table)
part1_df<-fread("http://bit.ly/CarreFourDataset")
head(part1_df)
```
#### Checking for missing values 

```{r}
# Checking for missing values
any(is.na.data.frame(part1_df))
# There are no missing values
```
#### Checking for duplicated values

```{r}
# Checing for duplicated data
any(duplicated.data.frame(part1_df))
# There are no duplicates in our dataset
```
#### Encoding categorical variables to nominal to perform PCA
```{r}
library("dbplyr")

numerical<-select_if(part1_df,is.numeric)
numerical
```

```{r}
?dummyVars
```

```{r}
# Selecting non numeric columns
head(part1_df)
# Caret package for dummy variables
library(caret)
# Encoding using Dummy variables and excluding unique ID and date time data
dums<-dummyVars("~.",data=part1_df[,c(-1,-9,-10)])
dums
# Encoding.
new_df<-data.frame(predict(dums,newdata =part1_df[,c(-1,-9,-10)]))
new_df
```
```{r}
?prcomp
```

```{r}
# Dimensionality Reduction technique
# Will use PCA so as to understand the variance displayed by each feature
reduced_df<-prcomp(new_df,center = TRUE)
# Since prcomp uses single varue decomposition that tests each points covariance and correlarion to each other.
reduced_df$sdev
# Checking the standard deviation of each PC
plot(reduced_df$sdev,main="Standard deviation of each Principal Component",ylab = "Standard Deviation",xlab = "Princial Components",type = "bar",col="blue")
```
### We can observe that the first three Principal components have a significant standard deviation in our dataset

```{r}
# Getting the sum of square distances from the projected point in our data
eigen_values<-get_eigenvalue(reduced_df)
eigen_values
# We get to understand from these that dimension 1 or PCA 1 explains almost 99 % of all the variance in my dataset
```
```{r}
summary(reduced_df)
```
### An importanct factpr was noted that in formulating dummy variables it recreated varibales with no variablility ie ranging between 0 and 1 and since PCA maximized on utilizing variablities , i am resulted to embarking on my numerical variables to carry out PCA effectively.

```{r}
?prcomp
```

```{r}
# This set seemed to have an anomaly as it was preventing scalability in the pca function. there is no variablility in this column
unique(part1_df$`gross margin percentage`)
```
### New PCA 

```{r}
# Columns that are numeric are more thus i will exclude non numeric columns
pca_d<-prcomp(part1_df[,c(6,7,8,12,14,15,16)],scale. = TRUE)
summary(pca_d)
# PC1 explains about 70% of variation in our dataset followed by PC2
```

```{r}
# Plotting a scree plot of the Principle comonents explained variance
library(factoextra)
fviz_eig(pca_d)
# From the scree plot above we can see that only PC1 ,PC2 and PC3 contain core information about our set that we will concentrate on that
```
```{r}
# Getting the variables that contributed to the principle components
library(ggbiplot)
ggbiplot(pca_d,obs.scale = 1,var.scale = 1,varname.adjust = 0.6,circle = TRUE)
# Rating contributes positively to PC1 which holds the core information of our dataset 
# Most variables are clustered together at negative value of PC1
```
```{r}
str(part1_df)
```


```{r}
# Getting the distribution of our categorical columns in the reduced dimension 
ggbiplot(pca_d,obs.scale = 1,var.scale = 1,varname.adjust = 0.6,circle = TRUE,groups =part1_df$Payment)
# Payment by Credit card and Ewallet is rampant and seems to be heavily clustered across core information(one with the highest variation) in the dataset
```
```{r}
# an attempt to extract information from the lower principal components
ggbiplot(pca_d,choice=c(3,4),obs.scale = 1,var.scale = 1,varname.adjust = 0.6,circle = TRUE,groups =part1_df$Payment)
# Its not easily interpratable.
```
## Feature Selection
#### This is ment to extract important features from our dataset using various metrics
```{r}
head(part1_df)

```




















