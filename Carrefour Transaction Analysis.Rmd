---
title: "Carrefour Kenya"
author: "Christine Muthee"
date: "12/3/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Marketing Analysis.

## 1. Main objective: 
#### * Getting the most relevant marketting strategies that will result in the highest number of sales ( Total price tax inclusive)
### Specific Objectives
#### * Reduce highly dimensional datasets to lower dimensions for easier interpratability and analysis while assessing the attributes carrying core information in our dataset.
#### * Getting important attributes from the dataset.
#### * Getting associations within the transactions that will uncover important relationships for effective marketing.
#### * Checking for anomalies in the sales dataset for the sole purpose of fraud detection.

## 2. Data Inspection.
```{r}
library(data.table)
part1_df<-fread("http://bit.ly/CarreFourDataset")
head(part1_df)
```
#### Checking for missing values 

```{r}
# Checking for missing values
any(is.na.data.frame(part1_df))
# There are no missing values
```
#### Checking for duplicated values

```{r}
# Checing for duplicated data
any(duplicated.data.frame(part1_df))
# There are no duplicates in our dataset
```
#### Encoding categorical variables to nominal to perform PCA
```{r}
library("dbplyr")

numerical<-select_if(part1_df,is.numeric)
numerical
```

```{r}
?dummyVars
```

```{r}
# Selecting non numeric columns
head(part1_df)
# Caret package for dummy variables
library(caret)
# Encoding using Dummy variables and excluding unique ID and date time data
dums<-dummyVars("~.",data=part1_df[,c(-1,-9,-10)])
dums
# Encoding.
new_df<-data.frame(predict(dums,newdata =part1_df[,c(-1,-9,-10)]))
new_df
```
```{r}
?prcomp
```

```{r}
# Dimensionality Reduction technique
# Will use PCA so as to understand the variance displayed by each feature
reduced_df<-prcomp(new_df,center = TRUE)
# Since prcomp uses single varue decomposition that tests each points covariance and correlarion to each other.
reduced_df$sdev
# Checking the standard deviation of each PC
plot(reduced_df$sdev,main="Standard deviation of each Principal Component",ylab = "Standard Deviation",xlab = "Princial Components",type = "bar",col="blue")
```
### We can observe that the first three Principal components have a significant standard deviation in our dataset

```{r}
# Getting the sum of square distances from the projected point in our data
eigen_values<-get_eigenvalue(reduced_df)
eigen_values
# We get to understand from these that dimension 1 or PCA 1 explains almost 99 % of all the variance in my dataset
```
```{r}
summary(reduced_df)
```
### An importanct factpr was noted that in formulating dummy variables it recreated varibales with no variablility ie ranging between 0 and 1 and since PCA maximized on utilizing variablities , i am resulted to embarking on my numerical variables to carry out PCA effectively.

```{r}
?prcomp
```

```{r}
# This set seemed to have an anomaly as it was preventing scalability in the pca function. there is no variablility in this column
unique(part1_df$`gross margin percentage`)
```
### New PCA 

```{r}
# Columns that are numeric are more thus i will exclude non numeric columns
pca_d<-prcomp(part1_df[,c(6,7,8,12,14,15,16)],scale. = TRUE)
summary(pca_d)
# PC1 explains about 70% of variation in our dataset followed by PC2
```

```{r}
# Plotting a scree plot of the Principle comonents explained variance
library(factoextra)
fviz_eig(pca_d)
# From the scree plot above we can see that only PC1 ,PC2 and PC3 contain core information about our set that we will concentrate on that
```
```{r}
# Getting the variables that contributed to the principle components
library(ggbiplot)
ggbiplot(pca_d,obs.scale = 1,var.scale = 1,varname.adjust = 0.6,circle = TRUE)
# Rating contributes positively to PC1 which holds the core information of our dataset 
# Most variables are clustered together at negative value of PC1
```
```{r}
str(part1_df)
```


```{r}
# Getting the distribution of our categorical columns in the reduced dimension 
ggbiplot(pca_d,obs.scale = 1,var.scale = 1,varname.adjust = 0.6,circle = TRUE,groups =part1_df$Payment)
# Payment by Credit card and Ewallet is rampant and seems to be heavily clustered across core information(one with the highest variation) in the dataset
```
```{r}
# an attempt to extract information from the lower principal components
ggbiplot(pca_d,choice=c(3,4),obs.scale = 1,var.scale = 1,varname.adjust = 0.6,circle = TRUE,groups =part1_df$Payment)
# Its not easily interpratable.
```
## Feature Selection
```{r}
?ewkm
```

Performing elbow method to find appropriate number of clusters so as not to get the correct weights of the Entropy weghted Kmeans feature selection function
```{r}
# Normalizing so as to perform cluster based feature selection using min max scaller
normalize<-function(x){
  return ((x-min(x))/(max(x)-min(x)))}
```

#### Normalizing Features
```{r}
norm_df<-as.data.frame(lapply(new_df, normalize))
summary(norm_df)
```


```{r}
# Using the encoded set of data excluding the gross margin which is non variant. 4 are the optimum clusters
fviz_nbclust(norm_df[,c(-21)],FUNcluster = kmeans,method = "wss") 
```
# Using Embedding methods: Entropy Weighted K means

```{r}
library(wskm)
#Setting the intial clusters as 3 first and a variable for weight distribution
# We get to see the importance of every varaible to the kmeans cluster
# We will exclude the gross margin percentage as its inclusion would give us errors in distance metrics.

my_model<-ewkm(norm_df[,c(-21)],2,lambda = 2,maxiter=1000)
my_model
```

```{r}
library(cluster)
# Plotting the cluster with 2 as my maximum clusters
fviz_cluster(my_model,data=norm_df[,c(-21)])
```


```{r}
# We get to the the importance of each parameter to the individual clusters
(my_model$weights)*10000
```
#### Important variables to cluster one are
* cogs
* income
* Total
* Tax

#### Important variables to cluster two are
* Price
* Quantity
* gross Income
* Total

### Association analysis
The aim of this is to find the relationship in the transactions below to optimize supermarket purchases

```{r}
# Loading the dataset for association
library(arules)
path_trans<-"http://bit.ly/SupermarketDatasetII"

assoc_df<-read.transactions(path_trans,sep=',')
assoc_df
``` 

```{r}
# Previewing a quick summary of my dataset to understand the purchases
summary(assoc_df)
# Mineral water eggs spaghetti,fries seem to be leading in popularity (as individual items)
```
```{r}
?itemFrequencyPlot
```

```{r}
# Plotting item frequency considering the top 20 items
par(mfcol=c(2,1))
itemFrequencyPlot(assoc_df,topN=20,col="blue",ylab="Item frequency",main=" Item Frequency Plots")
itemFrequencyPlot(assoc_df,support=0.09,col="darkblue",ylab="Frequency > 0.1 support")
# Mineral water is still taking the lead even with the minimum support at 0.09.
```
## Apriori algoritm to build association rules
```{r}
# The first rules 
rule1<-apriori(assoc_df,parameter = list(support=0.001,conf=0.8))
rule1
# With a restriction of a support of 0.001 (freq(X)/Total transactions) and a confidence of 80% the items were filtered to 74 items. We seem to have lost important rules as 74 is very little to work with
```
```{r}
?plot
```

```{r}
# Visualizing this in an association plot
library(arulesViz)
plot(rule1,type = "graph",control=list(type="items"))
# As much as we can see a positive correlation between confidence and support there are a few undisputable datapoints which are not popular but have a large confidence thus meaning they are bought in conjunction to other items(They are less likely to be bought alone).
```

```{r}
?apriori
```

```{r}
inspect(rule1[1:10])
# We are 95 % confident that for every transaction of {mushroom cream sauce, pasta}, excalope was bought along with it.
```

```{r}
# Minimizing support thershold alittle bit
rule2<-apriori(assoc_df,parameter =list(support=0.001,conf=0.75))
rule2
```
```{r}
# Viewing the rules extracted from these
inspect(rule2[1:20])
```
```{r}
# Mineral water seem to be standing out as a famous item. We'd want to know which items are bought before mineral water for us to maximuze the discounts on these products
sorted<-sort(rule2,by="confidence",decreasing = TRUE)
inspect(sorted[1:10])
```
```{r}
# Getting items purchased before mineral water
mineral<-subset(rule2,subset=rhs %pin% "mineral water")
# Sorting items by their confidence level
sorted_mineral<-sort(mineral,by="confidence",decreasing = TRUE)
# Viewing the top 10 items
inspect(sorted_mineral[1:10])
# Ground beef,olive oil and cake seem to be standing out
```
```{r}
# Getting items that are bought after eggs are bought
eggs<-subset(rule2,subset=lhs %pin% "eggs")
# Sorting items by their confidence level
sorted_eggs<-sort(eggs,by="confidence",decreasing = TRUE)
# Viewing the top 10 items
inspect(sorted_eggs[1:10])
# Mineral water is mostt likely to be bought when eggs are bought
```
## Anomaly detection

```{r}
# Loading the anomalize package to detect anomalies in the trends in our dataset
library(anomalize)
# This enables us to vizualize  these anomalies
library(tidyverse)

library(tibbletime)
```

```{r}
# Viweing the head of my dataset
anom_path<-"http://bit.ly/CarreFourSalesDataset"
carefour_anomaly_set<-read_csv(anom_path)%<%group_by(Sales)%<%as_tbl_time(date)

```
```{r}
anomalized<-anom_df%<%
  time_apply(Sales,merge=TRUE)%<%
  anomalize(remainder)%<%
  time_recompose()
anomalized%<%glimpse
```




































